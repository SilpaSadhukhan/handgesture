{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# *IMPORT AND INSTALL DEPENDENCIES*"],"metadata":{"id":"KFKdzr3BX2oF"}},{"cell_type":"code","source":["# Uninstall existing packages\n","!pip uninstall -y numpy pandas tensorflow opencv-python matplotlib mediapipe\n","\n","# Install all required packages at once\n","!pip install tensorflow opencv-python matplotlib mediapipe numpy pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARw5on-If8ay","executionInfo":{"status":"ok","timestamp":1756623865995,"user_tz":-330,"elapsed":216203,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}},"outputId":"36c561da-fb3f-48f9-e9b4-2e089512fc72"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 2.0.2\n","Uninstalling numpy-2.0.2:\n","  Successfully uninstalled numpy-2.0.2\n","Found existing installation: pandas 2.2.2\n","Uninstalling pandas-2.2.2:\n","  Successfully uninstalled pandas-2.2.2\n","Found existing installation: tensorflow 2.19.0\n","Uninstalling tensorflow-2.19.0:\n","  Successfully uninstalled tensorflow-2.19.0\n","Found existing installation: opencv-python 4.12.0.88\n","Uninstalling opencv-python-4.12.0.88:\n","  Successfully uninstalled opencv-python-4.12.0.88\n","Found existing installation: matplotlib 3.10.0\n","Uninstalling matplotlib-3.10.0:\n","  Successfully uninstalled matplotlib-3.10.0\n","\u001b[33mWARNING: Skipping mediapipe as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting tensorflow\n","  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n","Collecting opencv-python\n","  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n","Collecting matplotlib\n","  Downloading matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Collecting numpy\n","  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pandas\n","  Downloading pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n","Collecting tensorboard~=2.20.0 (from tensorflow)\n","  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","Collecting numpy\n","  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n","  Downloading mediapipe-0.10.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Collecting matplotlib\n","  Downloading matplotlib-3.10.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","INFO: pip is still looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n","  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","  Downloading matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","  Downloading matplotlib-3.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","  Downloading matplotlib-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","  Downloading matplotlib-3.9.1.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","  Downloading matplotlib-3.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","  Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","  Downloading matplotlib-3.8.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n","  Downloading matplotlib-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","  Downloading matplotlib-3.8.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","  Downloading matplotlib-3.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","  Downloading matplotlib-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n","  Downloading matplotlib-3.7.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n","  Downloading matplotlib-3.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n","  Downloading matplotlib-3.7.2.tar.gz (38.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyparsing<3.1,>=2.3.1 (from matplotlib)\n","  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n","INFO: pip is still looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","Collecting matplotlib\n","  Downloading matplotlib-3.7.1.tar.gz (38.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.7.0.tar.gz (36.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.6.3.tar.gz (35.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.6.2.tar.gz (35.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.6.1.tar.gz (35.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.6.0.tar.gz (35.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.5.3.tar.gz (35.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.5.2.tar.gz (35.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.5.1.tar.gz (35.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.5.0.tar.gz (35.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Downloading matplotlib-3.4.3.tar.gz (37.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","import mediapipe as mp\n","import cv2\n","import os"],"metadata":{"id":"Bga8C-HIbRps","executionInfo":{"status":"error","timestamp":1756623866006,"user_tz":-330,"elapsed":47,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}},"colab":{"base_uri":"https://localhost:8080/","height":399},"outputId":"a58fdde2-d149-44fb-c1d2-28c5bf9e8ea7"},"execution_count":2,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3788901091.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["# ***KEYPOINTS USING  MP HOLISTIC***\n","\n"],"metadata":{"id":"NlCWyAeTc2Hv"}},{"cell_type":"code","source":["mp_holistic = mp.solutions.holistic # Holistic model\n","mp_drawing = mp.solutions.drawing_utils # Drawing utilities"],"metadata":{"id":"V2PMEWw7wNS7","executionInfo":{"status":"aborted","timestamp":1756623866007,"user_tz":-330,"elapsed":43,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mediapipe_detection(image, model):\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n","    image.flags.writeable = False                  # Image is no longer writeable\n","    results = model.process(image)                 # Make prediction\n","    image.flags.writeable = True                   # Image is now writeable\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n","    return image, results"],"metadata":{"id":"v6_fhIEcwRIP","executionInfo":{"status":"aborted","timestamp":1756623866007,"user_tz":-330,"elapsed":40,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def draw_landmarks(image, results):\n","    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n","    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"],"metadata":{"id":"_Ft-5GiIwT3n","executionInfo":{"status":"aborted","timestamp":1756623866008,"user_tz":-330,"elapsed":40,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def draw_styled_landmarks(image, results):\n","    # Draw face connections\n","    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS,\n","                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n","                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n","                             )\n","    # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n","                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n","                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n","                             )\n","    # Draw left hand connections\n","    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n","                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n","                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n","                             )\n","    # Draw right hand connections\n","    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n","                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n","                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n","                             )"],"metadata":{"id":"rB5FFuMPwYZN","executionInfo":{"status":"aborted","timestamp":1756623866008,"user_tz":-330,"elapsed":39,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"status":"aborted","timestamp":1756623866008,"user_tz":-330,"elapsed":39,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}},"id":"bZlzCFdr0u4l"},"source":["import cv2\n","import matplotlib.pyplot as plt\n","import mediapipe as mp\n","\n","# Capture a single frame to demonstrate draw_landmarks\n","cap = cv2.VideoCapture(0)\n","\n","try:\n","    # Read a frame from the camera\n","    ret, frame = cap.read()\n","\n","    # Ensure frame was read successfully\n","    if ret:\n","        # Assuming 'holistic' is still defined from a previous cell where the model was initialized.\n","        # If not, you might need to re-initialize the holistic model here as well.\n","        # Also assuming 'mediapipe_detection' and 'draw_landmarks' are defined from previous cells.\n","\n","        # Initialize mediapipe holistic model if not already defined\n","        if 'holistic' not in locals() and 'holistic' not in globals():\n","             mp_holistic = mp.solutions.holistic\n","             holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n","        elif isinstance(holistic, mp.solutions.holistic.Holistic):\n","            # If holistic is defined but not as an instance of mp.solutions.holistic.Holistic, re-initialize\n","             if not isinstance(holistic, mp.solutions.holistic.Holistic):\n","                 mp_holistic = mp.solutions.holistic\n","                 holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n","\n","\n","        image, results = mediapipe_detection(frame, holistic)\n","\n","\n","        # Draw landmarks on the captured frame\n","        draw_landmarks(image, results)\n","\n","        # Display the frame with landmarks\n","        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","        plt.title(\"Frame with Landmarks\")\n","        plt.axis('off') # Hide axes\n","        plt.show()\n","    else:\n","        print(\"Failed to capture frame.\")\n","finally:\n","    # Release the video capture object\n","    if 'cap' in locals() and cap.isOpened():\n","        cap.release()\n","    # Close mediapipe holistic model if initialized in this block\n","    if 'holistic' in locals() and isinstance(holistic, mp.solutions.holistic.Holistic) and ('holistic' not in globals() or not isinstance(globals()['holistic'], mp.solutions.holistic.Holistic)):\n","         holistic.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***EXTRACT KEYPOINTS VALUES***"],"metadata":{"id":"ngk-lZicdH8r"}},{"cell_type":"code","source":["len(results.left_hand_landmarks.landmark)"],"metadata":{"id":"afaiV5zS1zeU","executionInfo":{"status":"aborted","timestamp":1756623866008,"user_tz":-330,"elapsed":39,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pose = []\n","for res in results.pose_landmarks.landmark:\n","    test = np.array([res.x, res.y, res.z, res.visibility])\n","    pose.append(test)"],"metadata":{"id":"xFaOdMCm2F8y","executionInfo":{"status":"aborted","timestamp":1756623866009,"user_tz":-330,"elapsed":40,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n","face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n","lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n","rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"],"metadata":{"id":"4Fgb3bN02HYZ","executionInfo":{"status":"aborted","timestamp":1756623866009,"user_tz":-330,"elapsed":39,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten()\n","    if results.face_landmarks\n","    else np.zeros(1404)\n"],"metadata":{"id":"HFTd0lyX2RSy","executionInfo":{"status":"aborted","timestamp":1756623866009,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_test = extract_keypoints(results)"],"metadata":{"id":"TwjgTRy-2VUQ","executionInfo":{"status":"aborted","timestamp":1756623866010,"user_tz":-330,"elapsed":39,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_test"],"metadata":{"id":"4ReuvUdK2XpW","executionInfo":{"status":"aborted","timestamp":1756623866010,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save('0', result_test)"],"metadata":{"id":"yL0CC3w42cXr","executionInfo":{"status":"aborted","timestamp":1756623866010,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.load('0.npy')"],"metadata":{"id":"VbuO-HXL2glX","executionInfo":{"status":"aborted","timestamp":1756623866011,"user_tz":-330,"elapsed":39,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***SETUP FOLDER FOR COLLECTION***"],"metadata":{"id":"AMqsw9-kdQ5q"}},{"cell_type":"code","source":["# Path for exported data, numpy arrays\n","DATA_PATH = os.path.join('MP_Data')\n","\n","# Actions that we try to detect\n","actions = np.array(['hello', 'thanks', 'iloveyou'])\n","\n","# Thirty videos worth of data\n","no_sequences = 30\n","\n","# Videos are going to be 30 frames in length\n","sequence_length = 30\n","\n","# Folder start\n","start_folder = 30"],"metadata":{"id":"bwn90mds2kBL","executionInfo":{"status":"aborted","timestamp":1756623866011,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for action in actions:\n","    dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n","    for sequence in range(1,no_sequences+1):\n","        try:\n","            os.makedirs(os.path.join(DATA_PATH, action, str(dirmax+sequence)))\n","        except:\n","            pass"],"metadata":{"id":"ToT1qPBb2omU","executionInfo":{"status":"aborted","timestamp":1756623866011,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***COLLECT KEYPOINTS VALUES FOR TRAINING AND TESTING***"],"metadata":{"id":"9QhrQGf2dZSB"}},{"cell_type":"code","source":["cap = cv2.VideoCapture(0)\n","# Set mediapipe model\n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","\n","    # NEW LOOP\n","    # Loop through actions\n","    for action in actions:\n","        # Loop through sequences aka videos\n","        for sequence in range(start_folder, start_folder+no_sequences):\n","            # Loop through video length aka sequence length\n","            for frame_num in range(sequence_length):\n","\n","                # Read feed\n","                ret, frame = cap.read()\n","\n","                # Make detections\n","                image, results = mediapipe_detection(frame, holistic)\n","\n","                # Draw landmarks\n","                draw_styled_landmarks(image, results)\n","\n","                # NEW Apply wait logic\n","                if frame_num == 0:\n","                    cv2.putText(image, 'STARTING COLLECTION', (120,200),\n","                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n","                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n","                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n","                    # Show to screen\n","                    cv2.imshow('OpenCV Feed', image)\n","                    cv2.waitKey(500)\n","                else:\n","                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n","                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n","                    # Show to screen\n","                    cv2.imshow('OpenCV Feed', image)\n","\n","                # NEW Export keypoints\n","                keypoints = extract_keypoints(results)\n","                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n","                np.save(npy_path, keypoints)\n","\n","                # Break gracefully\n","                if cv2.waitKey(10) & 0xFF == ord('q'):\n","                    break\n","\n","    cap.release()\n","    cv2.destroyAllWindows()"],"metadata":{"id":"9XsOJqzG2tLQ","executionInfo":{"status":"aborted","timestamp":1756623866011,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cap.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"e7KHnN6l26L_","executionInfo":{"status":"aborted","timestamp":1756623866011,"user_tz":-330,"elapsed":37,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***PREPROCESS DATA AND CREATE LABELS AND FEATURES***"],"metadata":{"id":"tIbiNRIGdk9d"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"UBV0wqAm28-r","executionInfo":{"status":"aborted","timestamp":1756623866011,"user_tz":-330,"elapsed":37,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_map = {label:num for num, label in enumerate(actions)}"],"metadata":{"id":"c-FwauKT2__f","executionInfo":{"status":"aborted","timestamp":1756623866012,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_map"],"metadata":{"id":"_fcpI6pR3IOD","executionInfo":{"status":"aborted","timestamp":1756623866012,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences, labels = [], []\n","for action in actions:\n","    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n","        window = []\n","        for frame_num in range(sequence_length):\n","            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n","            window.append(res)\n","        sequences.append(window)\n","        labels.append(label_map[action])"],"metadata":{"id":"k1HeorpF3Ljh","executionInfo":{"status":"aborted","timestamp":1756623866012,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["|np.array(sequences).shape"],"metadata":{"id":"jPYIz4Bl3RWz","executionInfo":{"status":"aborted","timestamp":1756623866012,"user_tz":-330,"elapsed":37,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fkI_zxir3UV9","executionInfo":{"status":"aborted","timestamp":1756623866012,"user_tz":-330,"elapsed":37,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.array(labels).shape"],"metadata":{"id":"7RDWqW_23T8A","executionInfo":{"status":"aborted","timestamp":1756623866013,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***BUILD AND TRAIN NEURAL NETWORK***"],"metadata":{"id":"CrF3aZQPdx1L"}},{"cell_type":"markdown","source":["# ***MAKE PREDICTION***"],"metadata":{"id":"iwrXqAdUeHx-"}},{"cell_type":"markdown","source":["# ***SAVE WEIGHTS***"],"metadata":{"id":"qkmPP0BDeNXA"}},{"cell_type":"markdown","source":["# ***EVALUATION USING CONFUSION MATRIX AND ACCURACY***"],"metadata":{"id":"j2CFPg9HeSo9"}},{"cell_type":"markdown","source":["# ***TEST IN REAL TIME***"],"metadata":{"id":"Z292wGvbedV-"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"DgTgazCveitI","executionInfo":{"status":"aborted","timestamp":1756623866013,"user_tz":-330,"elapsed":38,"user":{"displayName":"SILPA SADHUKHAN","userId":"12562614698143823666"}}},"execution_count":null,"outputs":[]}]}